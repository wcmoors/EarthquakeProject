{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and Linear Regression\n",
    "I am using a combination of Bill and Jessica's code to complete my analysis properly \n",
    "I am conducting PCA to see how variable interact within the Flinn-Engdahl regions and seeing if it is possible to do linear regression with these regions. I am also going to be adding B-values from Jessica's B-value notebook to the data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion\n",
    "\n",
    "The following cells will be used to ingest the data needed for the PCA and linear regression and reformat and reorganize them into dataframes that will be used in my analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enables the use of functions from other R notebooks \n",
    "source('initialize_data.R', echo = FALSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n",
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n",
      "Updating HTML index of packages in '.Library'\n",
      "Making 'packages.html' ... done\n",
      "── Attaching packages ─────────────────────────────────────── tidyverse 1.2.1 ──\n",
      "✔ ggplot2 3.1.1       ✔ purrr   0.3.2  \n",
      "✔ tibble  2.1.1       ✔ dplyr   0.8.0.1\n",
      "✔ tidyr   0.8.3       ✔ stringr 1.4.0  \n",
      "✔ readr   1.3.1       ✔ forcats 0.4.0  \n",
      "── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n",
      "✖ dplyr::filter() masks stats::filter()\n",
      "✖ dplyr::lag()    masks stats::lag()\n",
      "\n",
      "Attaching package: ‘data.table’\n",
      "\n",
      "The following objects are masked from ‘package:dplyr’:\n",
      "\n",
      "    between, first, last\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    transpose\n",
      "\n",
      "Loading required package: sp\n",
      "### Welcome to rworldmap ###\n",
      "For a short introduction type : \t vignette('rworldmap')\n",
      "Checking rgeos availability: TRUE\n",
      "\n",
      "Attaching package: ‘maps’\n",
      "\n",
      "The following object is masked from ‘package:purrr’:\n",
      "\n",
      "    map\n",
      "\n",
      "Google's Terms of Service: https://cloud.google.com/maps-platform/terms/.\n",
      "Please cite ggmap if you use it! See citation(\"ggmap\") for details.\n",
      "\n",
      "Attaching package: ‘plotly’\n",
      "\n",
      "The following object is masked from ‘package:ggmap’:\n",
      "\n",
      "    wind\n",
      "\n",
      "The following object is masked from ‘package:ggplot2’:\n",
      "\n",
      "    last_plot\n",
      "\n",
      "The following object is masked from ‘package:stats’:\n",
      "\n",
      "    filter\n",
      "\n",
      "The following object is masked from ‘package:graphics’:\n",
      "\n",
      "    layout\n",
      "\n",
      "rgdal: version: 1.4-3, (SVN revision 828)\n",
      " Geospatial Data Abstraction Library extensions to R successfully loaded\n",
      " Loaded GDAL runtime: GDAL 2.2.3, released 2017/11/20\n",
      " Path to GDAL shared files: /usr/share/gdal/2.2\n",
      " GDAL binary built with GEOS: TRUE \n",
      " Loaded PROJ.4 runtime: Rel. 4.9.3, 15 August 2016, [PJ_VERSION: 493]\n",
      " Path to PROJ.4 shared files: (autodetected)\n",
      " Linking to sp version: 1.3-1 \n",
      "rgeos version: 0.4-3, (SVN revision 595)\n",
      " GEOS runtime version: 3.6.2-CAPI-1.10.2 \n",
      " Linking to sp version: 1.3-1 \n",
      " Polygon checking: TRUE \n",
      "\n",
      "Loading required package: spData\n",
      "To access larger datasets in this package, install the spDataLarge\n",
      "package with: `install.packages('spDataLarge',\n",
      "repos='https://nowosad.github.io/drat/', type='source')`\n",
      "Loading required package: sf\n",
      "Linking to GEOS 3.6.2, GDAL 2.2.3, PROJ 4.9.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"libraries loaded successfully\"\n"
     ]
    }
   ],
   "source": [
    "#Installs necessary packages\n",
    "install.packages('spdep')\n",
    "\n",
    "load_libraries()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Files in \"~/jupyter/cs2019_Group11/GroupProducts/data\" loaded.\n",
      "Subset Completed.\n",
      "Parsed column: time.\n",
      "spatial df created\n"
     ]
    }
   ],
   "source": [
    "#Bill\n",
    "#Explicit location of main project data\n",
    "path <- \"~/jupyter/cs2019_Group11/GroupProducts/data\" \n",
    "\n",
    "#create df from main project data\n",
    "df <- loadFiles(path) %>%\n",
    "    reqCols %>%\n",
    "    parseDt('time')\n",
    "\n",
    "#create spatial point df\n",
    "dfsp <- tospdf(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OGR data source with driver: LIBKML \n",
      "Source: \"/dsa/home/ewghbb/jupyter/cs2019_Group11/GroupProducts/fe.kmz\", layer: \"fe\"\n",
      "with 754 features\n",
      "It has 12 fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in readOGR(tectonicdata):\n",
      "“Z-dimension discarded”"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n",
      "[1] \"none\"\n"
     ]
    }
   ],
   "source": [
    "#bill\n",
    "#takes about 10 minutes to run\n",
    "#adds Flinn-Engdahl regions to spatial point df and converts coordinate system\n",
    "tpts <-regions_data(dfsp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OGR data source with driver: LIBKML \n",
      "Source: \"/dsa/home/ewghbb/jupyter/cs2019_Group11/GroupProducts/fe.kmz\", layer: \"fe\"\n",
      "with 754 features\n",
      "It has 12 fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in readOGR(regiondata):\n",
      "“Z-dimension discarded”Regions defined for each Polygons\n",
      "Warning message:\n",
      "“Ignoring unknown parameters: fill”"
     ]
    }
   ],
   "source": [
    "#jessica \n",
    "base_world <- base_world_regions()\n",
    "#get base map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Jessica \n",
    "b.guten <-\n",
    "function(magn,m0=min(magn)){ #\n",
    "            x = subset(magn,subset=(magn>m0))\n",
    "            x = x-m0\n",
    "            n  = length(x)\n",
    "            beta = n/sum(x)\n",
    "            samplevar = beta/sum(x)\n",
    "    b  = beta/log(10)\n",
    "    se = sqrt(samplevar/log(10))\n",
    "            return(c(b,se))\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>depth</th><th scope=col>mag</th><th scope=col>time</th><th scope=col>id</th><th scope=col>updated</th><th scope=col>newname</th><th scope=col>latitude</th><th scope=col>longitude</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>  2.000            </td><td>2.6                </td><td>1999-07-01 23:29:27</td><td>usp0009awp         </td><td>2014-11-07 01:08:01</td><td>374                </td><td> 45.4500           </td><td>   3.790           </td></tr>\n",
       "\t<tr><td>  4.613            </td><td>3.1                </td><td>1999-07-01 22:10:51</td><td>uw10474128         </td><td>2016-07-23 18:59:57</td><td>6                  </td><td> 46.7105           </td><td>-122.778           </td></tr>\n",
       "\t<tr><td>106.800            </td><td>4.2                </td><td>1999-07-01 21:42:41</td><td>usp0009awk         </td><td>2014-11-07 01:08:01</td><td>1                  </td><td> 51.9950           </td><td> 177.972           </td></tr>\n",
       "\t<tr><td> 26.100            </td><td>3.9                </td><td>1999-07-01 20:50:11</td><td>usp0009awh         </td><td>2014-11-07 01:08:01</td><td>53                 </td><td>-31.7260           </td><td> -72.053           </td></tr>\n",
       "\t<tr><td>136.300            </td><td>3.1                </td><td>1999-07-01 19:52:19</td><td>usp0009awf         </td><td>2014-11-07 01:08:01</td><td>24                 </td><td> 59.8900           </td><td>-153.560           </td></tr>\n",
       "\t<tr><td> 30.200            </td><td>3.8                </td><td>1999-07-01 19:52:12</td><td>usp0009awe         </td><td>2014-11-07 01:08:01</td><td>741                </td><td> 36.0500           </td><td>  31.166           </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllll}\n",
       " depth & mag & time & id & updated & newname & latitude & longitude\\\\\n",
       "\\hline\n",
       "\t   2.000             & 2.6                 & 1999-07-01 23:29:27 & usp0009awp          & 2014-11-07 01:08:01 & 374                 &  45.4500            &    3.790           \\\\\n",
       "\t   4.613             & 3.1                 & 1999-07-01 22:10:51 & uw10474128          & 2016-07-23 18:59:57 & 6                   &  46.7105            & -122.778           \\\\\n",
       "\t 106.800             & 4.2                 & 1999-07-01 21:42:41 & usp0009awk          & 2014-11-07 01:08:01 & 1                   &  51.9950            &  177.972           \\\\\n",
       "\t  26.100             & 3.9                 & 1999-07-01 20:50:11 & usp0009awh          & 2014-11-07 01:08:01 & 53                  & -31.7260            &  -72.053           \\\\\n",
       "\t 136.300             & 3.1                 & 1999-07-01 19:52:19 & usp0009awf          & 2014-11-07 01:08:01 & 24                  &  59.8900            & -153.560           \\\\\n",
       "\t  30.200             & 3.8                 & 1999-07-01 19:52:12 & usp0009awe          & 2014-11-07 01:08:01 & 741                 &  36.0500            &   31.166           \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| depth | mag | time | id | updated | newname | latitude | longitude |\n",
       "|---|---|---|---|---|---|---|---|\n",
       "|   2.000             | 2.6                 | 1999-07-01 23:29:27 | usp0009awp          | 2014-11-07 01:08:01 | 374                 |  45.4500            |    3.790            |\n",
       "|   4.613             | 3.1                 | 1999-07-01 22:10:51 | uw10474128          | 2016-07-23 18:59:57 | 6                   |  46.7105            | -122.778            |\n",
       "| 106.800             | 4.2                 | 1999-07-01 21:42:41 | usp0009awk          | 2014-11-07 01:08:01 | 1                   |  51.9950            |  177.972            |\n",
       "|  26.100             | 3.9                 | 1999-07-01 20:50:11 | usp0009awh          | 2014-11-07 01:08:01 | 53                  | -31.7260            |  -72.053            |\n",
       "| 136.300             | 3.1                 | 1999-07-01 19:52:19 | usp0009awf          | 2014-11-07 01:08:01 | 24                  |  59.8900            | -153.560            |\n",
       "|  30.200             | 3.8                 | 1999-07-01 19:52:12 | usp0009awe          | 2014-11-07 01:08:01 | 741                 |  36.0500            |   31.166            |\n",
       "\n"
      ],
      "text/plain": [
       "  depth   mag time                id         updated             newname\n",
       "1   2.000 2.6 1999-07-01 23:29:27 usp0009awp 2014-11-07 01:08:01 374    \n",
       "2   4.613 3.1 1999-07-01 22:10:51 uw10474128 2016-07-23 18:59:57 6      \n",
       "3 106.800 4.2 1999-07-01 21:42:41 usp0009awk 2014-11-07 01:08:01 1      \n",
       "4  26.100 3.9 1999-07-01 20:50:11 usp0009awh 2014-11-07 01:08:01 53     \n",
       "5 136.300 3.1 1999-07-01 19:52:19 usp0009awf 2014-11-07 01:08:01 24     \n",
       "6  30.200 3.8 1999-07-01 19:52:12 usp0009awe 2014-11-07 01:08:01 741    \n",
       "  latitude longitude\n",
       "1  45.4500    3.790 \n",
       "2  46.7105 -122.778 \n",
       "3  51.9950  177.972 \n",
       "4 -31.7260  -72.053 \n",
       "5  59.8900 -153.560 \n",
       "6  36.0500   31.166 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Bill + jessica \n",
    "#converting spatial point df back to normal df in order to plot more meaningful visualizations\n",
    "tpts_df <- as.data.frame(tpts)\n",
    "tpts_df$latitude = df$latitude\n",
    "tpts_df$longitude = df$longitude\n",
    "tpts_df$coords.x1 = NULL\n",
    "tpts_df$coords.x2 = NULL\n",
    "\n",
    "head(tpts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "     depth             mag             time                    \n",
       " Min.   : -3.60   Min.   :2.500   Min.   :1999-01-01 01:02:39  \n",
       " 1st Qu.: 10.00   1st Qu.:2.990   1st Qu.:2004-12-08 03:12:53  \n",
       " Median : 22.00   Median :4.000   Median :2008-12-20 11:33:42  \n",
       " Mean   : 54.59   Mean   :3.794   Mean   :2009-08-02 09:43:03  \n",
       " 3rd Qu.: 50.00   3rd Qu.:4.500   3rd Qu.:2014-10-24 01:16:41  \n",
       " Max.   :735.80   Max.   :9.100   Max.   :2018-12-31 23:54:07  \n",
       "      id               updated                      newname         \n",
       " Length:511837      Min.   :2013-02-06 02:29:15   Length:511837     \n",
       " Class :character   1st Qu.:2014-11-07 01:25:49   Class :character  \n",
       " Mode  :character   Median :2014-11-07 01:48:07   Mode  :character  \n",
       "                    Mean   :2016-02-23 13:07:26                     \n",
       "                    3rd Qu.:2017-07-29 03:16:10                     \n",
       "                    Max.   :2019-06-28 18:47:47                     \n",
       "    latitude         longitude      \n",
       " Min.   :-84.422   Min.   :-180.00  \n",
       " 1st Qu.: -5.293   1st Qu.:-122.64  \n",
       " Median : 21.385   Median : -65.18  \n",
       " Mean   : 18.815   Mean   : -17.73  \n",
       " 3rd Qu.: 40.983   3rd Qu.: 120.06  \n",
       " Max.   : 87.092   Max.   : 180.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "summary(tpts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OGR data source with driver: LIBKML \n",
      "Source: \"/dsa/home/ewghbb/jupyter/cs2019_Group11/GroupProducts/fe.kmz\", layer: \"fe\"\n",
      "with 754 features\n",
      "It has 12 fields\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message in readOGR(tectonicdata):\n",
      "“Z-dimension discarded”"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Object of class SpatialPolygonsDataFrame\n",
       "Coordinates:\n",
       "   min max\n",
       "x -180 180\n",
       "y  -90  90\n",
       "Is projected: FALSE \n",
       "proj4string :\n",
       "[+proj=longlat +datum=WGS84 +no_defs +ellps=WGS84 +towgs84=0,0,0]\n",
       "Data attributes:\n",
       "     Name           description         timestamp            begin          \n",
       " Length:754         Length:754         Length:754         Length:754        \n",
       " Class :character   Class :character   Class :character   Class :character  \n",
       " Mode  :character   Mode  :character   Mode  :character   Mode  :character  \n",
       "                                                                            \n",
       "                                                                            \n",
       "                                                                            \n",
       "                                                                            \n",
       "     end            altitudeMode         tessellate    extrude    visibility\n",
       " Length:754         Length:754         Min.   :-1   Min.   :0   Min.   :-1  \n",
       " Class :character   Class :character   1st Qu.:-1   1st Qu.:0   1st Qu.:-1  \n",
       " Mode  :character   Mode  :character   Median :-1   Median :0   Median :-1  \n",
       "                                       Mean   :-1   Mean   :0   Mean   :-1  \n",
       "                                       3rd Qu.:-1   3rd Qu.:0   3rd Qu.:-1  \n",
       "                                       Max.   :-1   Max.   :0   Max.   :-1  \n",
       "                                                                            \n",
       "   drawOrder       icon             snippet                ID       \n",
       " Min.   : NA   Length:754         Length:754         Min.   :  1.0  \n",
       " 1st Qu.: NA   Class :character   Class :character   1st Qu.:189.2  \n",
       " Median : NA   Mode  :character   Mode  :character   Median :377.5  \n",
       " Mean   :NaN                                         Mean   :377.5  \n",
       " 3rd Qu.: NA                                         3rd Qu.:565.8  \n",
       " Max.   : NA                                         Max.   :754.0  \n",
       " NA's   :754                                                        "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Jessica\n",
    " tectonicdata = \"~/jupyter/cs2019_Group11/GroupProducts/fe.kmz\"\n",
    "    tectonicFeatures <- readOGR(tectonicdata)\n",
    "\n",
    "    #also transform data into same regions as the earthquake data for later comparisons\n",
    "    transTectonicFeatures <- spTransform(tectonicFeatures,  CRS(\"+init=epsg:4087\"))\n",
    "\n",
    "\n",
    "tectonicFeatures@data$ID <- seq.int(nrow(transTectonicFeatures))#this adds in an id which is the same \n",
    "#as the newname in the tpts to match regions with earthquakes\n",
    "\n",
    "summary(tectonicFeatures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>X</th><th scope=col>newname</th><th scope=col>depth</th><th scope=col>mag</th><th scope=col>time</th><th scope=col>id</th><th scope=col>updated</th><th scope=col>latitude</th><th scope=col>longitude</th><th scope=col>bvalue</th><th scope=col>error</th><th scope=col>time_lag</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>1                  </td><td> 10                </td><td> 33.0              </td><td>5.1                </td><td>1999-02-14 17:17:27</td><td>usp00092tz         </td><td>2014-11-07 01:07:01</td><td> 21.581            </td><td>-106.681           </td><td>0.6204207          </td><td>0.66570086         </td><td>NA                 </td></tr>\n",
       "\t<tr><td>2                  </td><td> 10                </td><td> 33.0              </td><td>4.5                </td><td>1999-03-16 06:14:07</td><td>usp00094kr         </td><td>2014-11-07 01:07:14</td><td> 22.002            </td><td>-107.375           </td><td>0.6204207          </td><td>0.66570086         </td><td>1999-02-14 17:17:27</td></tr>\n",
       "\t<tr><td>3                  </td><td> 10                </td><td> 10.0              </td><td>4.5                </td><td>1999-01-13 07:38:50</td><td>usp0009114         </td><td>2014-11-07 01:06:48</td><td> 22.690            </td><td>-107.988           </td><td>0.6204207          </td><td>0.66570086         </td><td>1999-03-16 06:14:07</td></tr>\n",
       "\t<tr><td>4                  </td><td> 10                </td><td> 10.0              </td><td>5.3                </td><td>1999-03-12 07:07:40</td><td>usp00094by         </td><td>2016-11-09 22:04:11</td><td> 22.219            </td><td>-107.368           </td><td>0.6204207          </td><td>0.66570086         </td><td>1999-01-13 07:38:50</td></tr>\n",
       "\t<tr><td>5                  </td><td> 10                </td><td> 33.0              </td><td>5.2                </td><td>1999-01-10 20:36:06</td><td>usp00090x3         </td><td>2016-11-09 21:38:36</td><td> 22.939            </td><td>-108.020           </td><td>0.6204207          </td><td>0.66570086         </td><td>1999-03-12 07:07:40</td></tr>\n",
       "\t<tr><td>6                  </td><td>100                </td><td>129.1              </td><td>4.3                </td><td>1999-04-15 14:05:21</td><td>usp00096bk         </td><td>2014-11-07 01:07:28</td><td>-19.295            </td><td> -69.268           </td><td>0.3347563          </td><td>0.04778564         </td><td>NA                 </td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       " X & newname & depth & mag & time & id & updated & latitude & longitude & bvalue & error & time\\_lag\\\\\n",
       "\\hline\n",
       "\t 1                   &  10                 &  33.0               & 5.1                 & 1999-02-14 17:17:27 & usp00092tz          & 2014-11-07 01:07:01 &  21.581             & -106.681            & 0.6204207           & 0.66570086          & NA                 \\\\\n",
       "\t 2                   &  10                 &  33.0               & 4.5                 & 1999-03-16 06:14:07 & usp00094kr          & 2014-11-07 01:07:14 &  22.002             & -107.375            & 0.6204207           & 0.66570086          & 1999-02-14 17:17:27\\\\\n",
       "\t 3                   &  10                 &  10.0               & 4.5                 & 1999-01-13 07:38:50 & usp0009114          & 2014-11-07 01:06:48 &  22.690             & -107.988            & 0.6204207           & 0.66570086          & 1999-03-16 06:14:07\\\\\n",
       "\t 4                   &  10                 &  10.0               & 5.3                 & 1999-03-12 07:07:40 & usp00094by          & 2016-11-09 22:04:11 &  22.219             & -107.368            & 0.6204207           & 0.66570086          & 1999-01-13 07:38:50\\\\\n",
       "\t 5                   &  10                 &  33.0               & 5.2                 & 1999-01-10 20:36:06 & usp00090x3          & 2016-11-09 21:38:36 &  22.939             & -108.020            & 0.6204207           & 0.66570086          & 1999-03-12 07:07:40\\\\\n",
       "\t 6                   & 100                 & 129.1               & 4.3                 & 1999-04-15 14:05:21 & usp00096bk          & 2014-11-07 01:07:28 & -19.295             &  -69.268            & 0.3347563           & 0.04778564          & NA                 \\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| X | newname | depth | mag | time | id | updated | latitude | longitude | bvalue | error | time_lag |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 1                   |  10                 |  33.0               | 5.1                 | 1999-02-14 17:17:27 | usp00092tz          | 2014-11-07 01:07:01 |  21.581             | -106.681            | 0.6204207           | 0.66570086          | NA                  |\n",
       "| 2                   |  10                 |  33.0               | 4.5                 | 1999-03-16 06:14:07 | usp00094kr          | 2014-11-07 01:07:14 |  22.002             | -107.375            | 0.6204207           | 0.66570086          | 1999-02-14 17:17:27 |\n",
       "| 3                   |  10                 |  10.0               | 4.5                 | 1999-01-13 07:38:50 | usp0009114          | 2014-11-07 01:06:48 |  22.690             | -107.988            | 0.6204207           | 0.66570086          | 1999-03-16 06:14:07 |\n",
       "| 4                   |  10                 |  10.0               | 5.3                 | 1999-03-12 07:07:40 | usp00094by          | 2016-11-09 22:04:11 |  22.219             | -107.368            | 0.6204207           | 0.66570086          | 1999-01-13 07:38:50 |\n",
       "| 5                   |  10                 |  33.0               | 5.2                 | 1999-01-10 20:36:06 | usp00090x3          | 2016-11-09 21:38:36 |  22.939             | -108.020            | 0.6204207           | 0.66570086          | 1999-03-12 07:07:40 |\n",
       "| 6                   | 100                 | 129.1               | 4.3                 | 1999-04-15 14:05:21 | usp00096bk          | 2014-11-07 01:07:28 | -19.295             |  -69.268            | 0.3347563           | 0.04778564          | NA                  |\n",
       "\n"
      ],
      "text/plain": [
       "  X newname depth mag time                id         updated            \n",
       "1 1  10      33.0 5.1 1999-02-14 17:17:27 usp00092tz 2014-11-07 01:07:01\n",
       "2 2  10      33.0 4.5 1999-03-16 06:14:07 usp00094kr 2014-11-07 01:07:14\n",
       "3 3  10      10.0 4.5 1999-01-13 07:38:50 usp0009114 2014-11-07 01:06:48\n",
       "4 4  10      10.0 5.3 1999-03-12 07:07:40 usp00094by 2016-11-09 22:04:11\n",
       "5 5  10      33.0 5.2 1999-01-10 20:36:06 usp00090x3 2016-11-09 21:38:36\n",
       "6 6 100     129.1 4.3 1999-04-15 14:05:21 usp00096bk 2014-11-07 01:07:28\n",
       "  latitude longitude bvalue    error      time_lag           \n",
       "1  21.581  -106.681  0.6204207 0.66570086 NA                 \n",
       "2  22.002  -107.375  0.6204207 0.66570086 1999-02-14 17:17:27\n",
       "3  22.690  -107.988  0.6204207 0.66570086 1999-03-16 06:14:07\n",
       "4  22.219  -107.368  0.6204207 0.66570086 1999-01-13 07:38:50\n",
       "5  22.939  -108.020  0.6204207 0.66570086 1999-03-12 07:07:40\n",
       "6 -19.295   -69.268  0.3347563 0.04778564 NA                 "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bvalue_df = read.csv(\"data.csv\",header=TRUE,sep=\",\")\n",
    "\n",
    "head(bvalue_df)\n",
    "# here the data created from the bvalue_model notebook will be loaded in order to save computational time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in format.default(df[[col]], \"%Y\"): invalid 'trim' argument\n",
     "output_type": "error",
     "traceback": [
      "Error in format.default(df[[col]], \"%Y\"): invalid 'trim' argument\nTraceback:\n",
      "1. parseDt(bvalue_df, \"time\")",
      "2. format(df[[col]], \"%Y\")",
      "3. format.default(df[[col]], \"%Y\")"
     ]
    }
   ],
   "source": [
    "#Elysa\n",
    "#Final Cleaning of data for PCA\n",
    "bvalue_df = parseDt(bvalue_df,'time')\n",
    "bvalue_df$time = NULL\n",
    "head(bvalue_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'data.frame':\t506200 obs. of  12 variables:\n",
      " $ X        : int  1 2 3 4 5 6 7 8 9 10 ...\n",
      " $ newname  : int  10 10 10 10 10 100 100 100 100 100 ...\n",
      " $ depth    : num  33 33 10 10 33 ...\n",
      " $ mag      : num  5.1 4.5 4.5 5.3 5.2 4.3 3.9 4.1 3.5 3.9 ...\n",
      " $ time     : chr  \"1999-02-14 17:17:27\" \"1999-03-16 06:14:07\" \"1999-01-13 07:38:50\" \"1999-03-12 07:07:40\" ...\n",
      " $ id       : chr  \"usp00092tz\" \"usp00094kr\" \"usp0009114\" \"usp00094by\" ...\n",
      " $ updated  : chr  \"2014-11-07 01:07:01\" \"2014-11-07 01:07:14\" \"2014-11-07 01:06:48\" \"2016-11-09 22:04:11\" ...\n",
      " $ latitude : num  21.6 22 22.7 22.2 22.9 ...\n",
      " $ longitude: num  -107 -107 -108 -107 -108 ...\n",
      " $ bvalue   : num  0.62 0.62 0.62 0.62 0.62 ...\n",
      " $ error    : num  0.666 0.666 0.666 0.666 0.666 ...\n",
      " $ time_lag : chr  NA \"1999-02-14 17:17:27\" \"1999-03-16 06:14:07\" \"1999-01-13 07:38:50\" ...\n"
     ]
    }
   ],
   "source": [
    "str(bvalue_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>newname</th><th scope=col>max_mag</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>197</td><td>9.1</td></tr>\n",
       "\t<tr><td>570</td><td>9.1</td></tr>\n",
       "\t<tr><td> 54</td><td>8.8</td></tr>\n",
       "\t<tr><td>593</td><td>8.6</td></tr>\n",
       "\t<tr><td> 65</td><td>8.4</td></tr>\n",
       "\t<tr><td>191</td><td>8.4</td></tr>\n",
       "\t<tr><td>171</td><td>8.3</td></tr>\n",
       "\t<tr><td>553</td><td>8.3</td></tr>\n",
       "\t<tr><td>669</td><td>8.3</td></tr>\n",
       "\t<tr><td> 60</td><td>8.2</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " newname & max\\_mag\\\\\n",
       "\\hline\n",
       "\t 197 & 9.1\\\\\n",
       "\t 570 & 9.1\\\\\n",
       "\t  54 & 8.8\\\\\n",
       "\t 593 & 8.6\\\\\n",
       "\t  65 & 8.4\\\\\n",
       "\t 191 & 8.4\\\\\n",
       "\t 171 & 8.3\\\\\n",
       "\t 553 & 8.3\\\\\n",
       "\t 669 & 8.3\\\\\n",
       "\t  60 & 8.2\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| newname | max_mag |\n",
       "|---|---|\n",
       "| 197 | 9.1 |\n",
       "| 570 | 9.1 |\n",
       "|  54 | 8.8 |\n",
       "| 593 | 8.6 |\n",
       "|  65 | 8.4 |\n",
       "| 191 | 8.4 |\n",
       "| 171 | 8.3 |\n",
       "| 553 | 8.3 |\n",
       "| 669 | 8.3 |\n",
       "|  60 | 8.2 |\n",
       "\n"
      ],
      "text/plain": [
       "   newname max_mag\n",
       "1  197     9.1    \n",
       "2  570     9.1    \n",
       "3   54     8.8    \n",
       "4  593     8.6    \n",
       "5   65     8.4    \n",
       "6  191     8.4    \n",
       "7  171     8.3    \n",
       "8  553     8.3    \n",
       "9  669     8.3    \n",
       "10  60     8.2    "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Bill + Elysa \n",
    "#finding top ten Flinn-Engdahl regions by max magnitude occuring in that region\n",
    "\n",
    "#grouping and summarising by max mag\n",
    "tpts_df_mm = bvalue_df %>% group_by(newname) %>% summarise(max(mag))\n",
    "\n",
    "#changing column name to max_mag for easier use\n",
    "colnames(tpts_df_mm) <- c('newname', 'max_mag')\n",
    "\n",
    "#ordering by descending max_mag\n",
    "tpts_df_mm = arrange(tpts_df_mm, desc(max_mag))\n",
    "\n",
    "#displaying top then regions and their max_mags\n",
    "top_ten_mag_regions = head(tpts_df_mm, 10)\n",
    "top_ten_mag_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th scope=col>newname</th><th scope=col>max_mag</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><td>518 </td><td>2.60</td></tr>\n",
       "\t<tr><td>597 </td><td>2.60</td></tr>\n",
       "\t<tr><td>430 </td><td>2.70</td></tr>\n",
       "\t<tr><td>478 </td><td>2.80</td></tr>\n",
       "\t<tr><td>400 </td><td>2.90</td></tr>\n",
       "\t<tr><td>622 </td><td>2.90</td></tr>\n",
       "\t<tr><td>425 </td><td>2.95</td></tr>\n",
       "\t<tr><td>740 </td><td>2.98</td></tr>\n",
       "\t<tr><td>384 </td><td>3.00</td></tr>\n",
       "\t<tr><td>432 </td><td>3.00</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|ll}\n",
       " newname & max\\_mag\\\\\n",
       "\\hline\n",
       "\t 518  & 2.60\\\\\n",
       "\t 597  & 2.60\\\\\n",
       "\t 430  & 2.70\\\\\n",
       "\t 478  & 2.80\\\\\n",
       "\t 400  & 2.90\\\\\n",
       "\t 622  & 2.90\\\\\n",
       "\t 425  & 2.95\\\\\n",
       "\t 740  & 2.98\\\\\n",
       "\t 384  & 3.00\\\\\n",
       "\t 432  & 3.00\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| newname | max_mag |\n",
       "|---|---|\n",
       "| 518  | 2.60 |\n",
       "| 597  | 2.60 |\n",
       "| 430  | 2.70 |\n",
       "| 478  | 2.80 |\n",
       "| 400  | 2.90 |\n",
       "| 622  | 2.90 |\n",
       "| 425  | 2.95 |\n",
       "| 740  | 2.98 |\n",
       "| 384  | 3.00 |\n",
       "| 432  | 3.00 |\n",
       "\n"
      ],
      "text/plain": [
       "   newname max_mag\n",
       "1  518     2.60   \n",
       "2  597     2.60   \n",
       "3  430     2.70   \n",
       "4  478     2.80   \n",
       "5  400     2.90   \n",
       "6  622     2.90   \n",
       "7  425     2.95   \n",
       "8  740     2.98   \n",
       "9  384     3.00   \n",
       "10 432     3.00   "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Elysa \n",
    "#creates the bottom ten regions\n",
    "tpts_df_mm2 = arrange(tpts_df_mm,max_mag)\n",
    "bottom_ten_mag_regions = head(tpts_df_mm2, 10)\n",
    "bottom_ten_mag_regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<thead><tr><th></th><th scope=col>X</th><th scope=col>newname</th><th scope=col>depth</th><th scope=col>mag</th><th scope=col>time</th><th scope=col>id</th><th scope=col>updated</th><th scope=col>latitude</th><th scope=col>longitude</th><th scope=col>bvalue</th><th scope=col>error</th><th scope=col>time_lag</th></tr></thead>\n",
       "<tbody>\n",
       "\t<tr><th scope=row>3050</th><td>3050               </td><td>171                </td><td>33.0               </td><td>4.7                </td><td>1999-05-19 06:31:33</td><td>usp00098am         </td><td>2014-11-07 01:07:41</td><td>46.786             </td><td>151.835            </td><td>0.2822652          </td><td>0.01414427         </td><td>NA                 </td></tr>\n",
       "\t<tr><th scope=row>3051</th><td>3051               </td><td>171                </td><td>72.6               </td><td>4.7                </td><td>1999-07-22 08:57:54</td><td>usp0009byj         </td><td>2014-11-07 01:08:07</td><td>43.883             </td><td>147.124            </td><td>0.2822652          </td><td>0.01414427         </td><td>1999-05-19 06:31:33</td></tr>\n",
       "\t<tr><th scope=row>3052</th><td>3052               </td><td>171                </td><td>45.3               </td><td>4.6                </td><td>1999-08-29 21:12:52</td><td>usp0009dt8         </td><td>2014-11-07 01:08:21</td><td>46.655             </td><td>152.748            </td><td>0.2822652          </td><td>0.01414427         </td><td>1999-07-22 08:57:54</td></tr>\n",
       "\t<tr><th scope=row>3053</th><td>3053               </td><td>171                </td><td>48.0               </td><td>4.8                </td><td>1999-05-28 19:50:28</td><td>usp00098xn         </td><td>2014-11-07 01:07:43</td><td>45.340             </td><td>151.272            </td><td>0.2822652          </td><td>0.01414427         </td><td>1999-08-29 21:12:52</td></tr>\n",
       "\t<tr><th scope=row>3054</th><td>3054               </td><td>171                </td><td>33.0               </td><td>5.3                </td><td>1999-02-14 11:22:37</td><td>usp00092t8         </td><td>2016-11-09 21:53:03</td><td>44.506             </td><td>149.710            </td><td>0.2822652          </td><td>0.01414427         </td><td>1999-05-28 19:50:28</td></tr>\n",
       "\t<tr><th scope=row>3055</th><td>3055               </td><td>171                </td><td>33.0               </td><td>4.5                </td><td>1999-06-20 03:35:03</td><td>usp0009a6t         </td><td>2014-11-07 01:07:55</td><td>46.304             </td><td>151.618            </td><td>0.2822652          </td><td>0.01414427         </td><td>1999-02-14 11:22:37</td></tr>\n",
       "</tbody>\n",
       "</table>\n"
      ],
      "text/latex": [
       "\\begin{tabular}{r|llllllllllll}\n",
       "  & X & newname & depth & mag & time & id & updated & latitude & longitude & bvalue & error & time\\_lag\\\\\n",
       "\\hline\n",
       "\t3050 & 3050                & 171                 & 33.0                & 4.7                 & 1999-05-19 06:31:33 & usp00098am          & 2014-11-07 01:07:41 & 46.786              & 151.835             & 0.2822652           & 0.01414427          & NA                 \\\\\n",
       "\t3051 & 3051                & 171                 & 72.6                & 4.7                 & 1999-07-22 08:57:54 & usp0009byj          & 2014-11-07 01:08:07 & 43.883              & 147.124             & 0.2822652           & 0.01414427          & 1999-05-19 06:31:33\\\\\n",
       "\t3052 & 3052                & 171                 & 45.3                & 4.6                 & 1999-08-29 21:12:52 & usp0009dt8          & 2014-11-07 01:08:21 & 46.655              & 152.748             & 0.2822652           & 0.01414427          & 1999-07-22 08:57:54\\\\\n",
       "\t3053 & 3053                & 171                 & 48.0                & 4.8                 & 1999-05-28 19:50:28 & usp00098xn          & 2014-11-07 01:07:43 & 45.340              & 151.272             & 0.2822652           & 0.01414427          & 1999-08-29 21:12:52\\\\\n",
       "\t3054 & 3054                & 171                 & 33.0                & 5.3                 & 1999-02-14 11:22:37 & usp00092t8          & 2016-11-09 21:53:03 & 44.506              & 149.710             & 0.2822652           & 0.01414427          & 1999-05-28 19:50:28\\\\\n",
       "\t3055 & 3055                & 171                 & 33.0                & 4.5                 & 1999-06-20 03:35:03 & usp0009a6t          & 2014-11-07 01:07:55 & 46.304              & 151.618             & 0.2822652           & 0.01414427          & 1999-02-14 11:22:37\\\\\n",
       "\\end{tabular}\n"
      ],
      "text/markdown": [
       "\n",
       "| <!--/--> | X | newname | depth | mag | time | id | updated | latitude | longitude | bvalue | error | time_lag |\n",
       "|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
       "| 3050 | 3050                | 171                 | 33.0                | 4.7                 | 1999-05-19 06:31:33 | usp00098am          | 2014-11-07 01:07:41 | 46.786              | 151.835             | 0.2822652           | 0.01414427          | NA                  |\n",
       "| 3051 | 3051                | 171                 | 72.6                | 4.7                 | 1999-07-22 08:57:54 | usp0009byj          | 2014-11-07 01:08:07 | 43.883              | 147.124             | 0.2822652           | 0.01414427          | 1999-05-19 06:31:33 |\n",
       "| 3052 | 3052                | 171                 | 45.3                | 4.6                 | 1999-08-29 21:12:52 | usp0009dt8          | 2014-11-07 01:08:21 | 46.655              | 152.748             | 0.2822652           | 0.01414427          | 1999-07-22 08:57:54 |\n",
       "| 3053 | 3053                | 171                 | 48.0                | 4.8                 | 1999-05-28 19:50:28 | usp00098xn          | 2014-11-07 01:07:43 | 45.340              | 151.272             | 0.2822652           | 0.01414427          | 1999-08-29 21:12:52 |\n",
       "| 3054 | 3054                | 171                 | 33.0                | 5.3                 | 1999-02-14 11:22:37 | usp00092t8          | 2016-11-09 21:53:03 | 44.506              | 149.710             | 0.2822652           | 0.01414427          | 1999-05-28 19:50:28 |\n",
       "| 3055 | 3055                | 171                 | 33.0                | 4.5                 | 1999-06-20 03:35:03 | usp0009a6t          | 2014-11-07 01:07:55 | 46.304              | 151.618             | 0.2822652           | 0.01414427          | 1999-02-14 11:22:37 |\n",
       "\n"
      ],
      "text/plain": [
       "     X    newname depth mag time                id         updated            \n",
       "3050 3050 171     33.0  4.7 1999-05-19 06:31:33 usp00098am 2014-11-07 01:07:41\n",
       "3051 3051 171     72.6  4.7 1999-07-22 08:57:54 usp0009byj 2014-11-07 01:08:07\n",
       "3052 3052 171     45.3  4.6 1999-08-29 21:12:52 usp0009dt8 2014-11-07 01:08:21\n",
       "3053 3053 171     48.0  4.8 1999-05-28 19:50:28 usp00098xn 2014-11-07 01:07:43\n",
       "3054 3054 171     33.0  5.3 1999-02-14 11:22:37 usp00092t8 2016-11-09 21:53:03\n",
       "3055 3055 171     33.0  4.5 1999-06-20 03:35:03 usp0009a6t 2014-11-07 01:07:55\n",
       "     latitude longitude bvalue    error      time_lag           \n",
       "3050 46.786   151.835   0.2822652 0.01414427 NA                 \n",
       "3051 43.883   147.124   0.2822652 0.01414427 1999-05-19 06:31:33\n",
       "3052 46.655   152.748   0.2822652 0.01414427 1999-07-22 08:57:54\n",
       "3053 45.340   151.272   0.2822652 0.01414427 1999-08-29 21:12:52\n",
       "3054 44.506   149.710   0.2822652 0.01414427 1999-05-28 19:50:28\n",
       "3055 46.304   151.618   0.2822652 0.01414427 1999-02-14 11:22:37"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>35942</li>\n",
       "\t<li>12</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 35942\n",
       "\\item 12\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 35942\n",
       "2. 12\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       "[1] 35942    12"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Elysa\n",
    "#Creating a new dataframe with only the top ten regions \n",
    "keep <- c(\"197\",\"570\",\"54\",\"593\",\"191\",\"65\",\"171\",\"553\",\"669\",\"60\")\n",
    "tpts_df = bvalue_df[bvalue_df$newname %in% keep, ]\n",
    "head(tpts_df)\n",
    "dim(tpts_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<ol class=list-inline>\n",
       "\t<li>197</li>\n",
       "\t<li>570</li>\n",
       "\t<li>54</li>\n",
       "\t<li>593</li>\n",
       "\t<li>65</li>\n",
       "\t<li>191</li>\n",
       "\t<li>171</li>\n",
       "\t<li>553</li>\n",
       "\t<li>669</li>\n",
       "\t<li>60</li>\n",
       "</ol>\n"
      ],
      "text/latex": [
       "\\begin{enumerate*}\n",
       "\\item 197\n",
       "\\item 570\n",
       "\\item 54\n",
       "\\item 593\n",
       "\\item 65\n",
       "\\item 191\n",
       "\\item 171\n",
       "\\item 553\n",
       "\\item 669\n",
       "\\item 60\n",
       "\\end{enumerate*}\n"
      ],
      "text/markdown": [
       "1. 197\n",
       "2. 570\n",
       "3. 54\n",
       "4. 593\n",
       "5. 65\n",
       "6. 191\n",
       "7. 171\n",
       "8. 553\n",
       "9. 669\n",
       "10. 60\n",
       "\n",
       "\n"
      ],
      "text/plain": [
       " [1] 197 570  54 593  65 191 171 553 669  60"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Elysa\n",
    "#Finds the unique values of the highest magnitude regions in the dataframe \n",
    "regions = unique(top_ten_mag_regions$newname)\n",
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in `$<-.data.frame`(`*tmp*`, time_Time, value = numeric(0)): replacement has 0 rows, data has 506200\n",
     "output_type": "error",
     "traceback": [
      "Error in `$<-.data.frame`(`*tmp*`, time_Time, value = numeric(0)): replacement has 0 rows, data has 506200\nTraceback:\n",
      "1. `$<-`(`*tmp*`, time_Time, value = numeric(0))",
      "2. `$<-.data.frame`(`*tmp*`, time_Time, value = numeric(0))",
      "3. stop(sprintf(ngettext(N, \"replacement has %d row, data has %d\", \n .     \"replacement has %d rows, data has %d\"), N, nrows), domain = NA)"
     ]
    }
   ],
   "source": [
    "#Final check of the data and then creating more time variables\n",
    "bvalue_df$bvalue <- as.numeric(bvalue_df$bvalue)\n",
    "bvalue_df$time_Time <- as.numeric(bvalue_df$time_Time)\n",
    "str(bvalue_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elysa\n",
    "#Testing to see if I can create dataframes basing on unique regions using a for loop to save time \n",
    "#After the dataframes are seperated by region then I can create a PCA for each one \n",
    "for (i in c(1,2,3,4,5,6,7,8,9,10)) {\n",
    "  assign(paste0(\"df_\", i), subset(bvalue_df, newname == regions[i],select=c(depth,mag,time_Year,time_Time)) )\n",
    "}\n",
    "\n",
    "dflist = list(df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8,df_9,df_9,df_10)\n",
    "\n",
    "for (i in 1:length(dflist)) {\n",
    "  assign(paste0(\"PCA_\", i),prcomp(dflist[[i]], center = TRUE,scale. = TRUE ))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Importance of components:\n",
       "                          PC1    PC2    PC3   PC4\n",
       "Standard deviation     1.1316 1.0061 0.9855 0.858\n",
       "Proportion of Variance 0.3201 0.2530 0.2428 0.184\n",
       "Cumulative Proportion  0.3201 0.5732 0.8160 1.000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Elysa\n",
    "#Testing to see if the PCA was created for all ten values \n",
    "summary(PCA_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elysa\n",
    "#Created a base function to create PCA based on region\n",
    "#Region can be changed to continent or whatever variable you'd like to use \n",
    "PCAFunc = function(df){\n",
    "    regions = unique(df$newname)\n",
    "regions\n",
    "for (i in c(1,2,3,4,5,6,7,8,9,10)) {\n",
    "  assign(paste0(\"df_\", i), subset(df, newname == regions[i],select=c(newname,depth,mag,time_Year,time_Time)) )\n",
    "}\n",
    "\n",
    "dflist = list(df_1,df_2,df_3,df_4,df_5,df_6,df_7,df_8,df_9,df_9,df_10)\n",
    "\n",
    "for (i in 1:length(dflist)) {\n",
    "  assign(paste0(\"PCA_\", i),prcomp(dflist[[i]], center = TRUE ))\n",
    "}\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "ERROR",
     "evalue": "Error in colMeans(x, na.rm = TRUE): 'x' must be numeric\n",
     "output_type": "error",
     "traceback": [
      "Error in colMeans(x, na.rm = TRUE): 'x' must be numeric\nTraceback:\n",
      "1. PCAFunc(bvalue_df)",
      "2. assign(paste0(\"PCA_\", i), prcomp(dflist[[i]], center = TRUE))   # at line 14 of file <text>",
      "3. prcomp(dflist[[i]], center = TRUE)   # at line 14 of file <text>",
      "4. prcomp.default(dflist[[i]], center = TRUE)",
      "5. scale(x, center = center, scale = scale.)",
      "6. scale.default(x, center = center, scale = scale.)",
      "7. colMeans(x, na.rm = TRUE)"
     ]
    }
   ],
   "source": [
    "#testing to see if my PCA function worked \n",
    "PCAFunc(bvalue_df)\n",
    "summary(PCA_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries to visualize the PCA\n",
    "library(devtools)\n",
    "options(unzip = \"internal\")\n",
    "devtools::install_github(\"vqv/ggbiplot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the library for plotting PCA \n",
    "library(ggbiplot)\n",
    "\n",
    "#Created a list of PCAs for each top ten highest magnitude region to iterate through\n",
    "pcalist = list(PCA_1,PCA_2,PCA_3,PCA_4,PCA_5,PCA_6,PCA_7,PCA_8,PCA_9,PCA_10)\n",
    "\n",
    "for (i in 1:length(pcalist)) {\n",
    "  print(ggbiplot(pcalist[[i]],alpha = 0.05))\n",
    "}\n",
    "\n",
    "#PC1 is Depth and PC2 is Mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the addition of the time variable we can see how the explanation of variances changes from region to region. \n",
    "B-values could not be included in the PCA due to each region having the same b-value making the variance equal to 0.\n",
    "Now I will look at the PCA for the bottom most regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elysa\n",
    "#Creating a new dataframe with only the bottom ten regions \n",
    "keep <- c(\"518 \",\"597 \",\"430\",\"478\",\"400\",\"622\",\"425\",\"740\",\"384\",\"387\")\n",
    "tpts_df2 = bvalue_df[bvalue_df$newname %in% keep, ]\n",
    "tpts_df2[is.na(tpts_df2)] <- 0\n",
    "tpts_df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elysa\n",
    "#Finds the unique values of the lowest magnitude regions in the dataframe \n",
    "regions = unique(bottom_ten_mag_regions$newname)\n",
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to make sure time is an integer value so it works for PCA\n",
    "tpts_df2$time_Time <- as.numeric(tpts_df2$time_Time)\n",
    "str(tpts_df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing to see if my PCA function worked \n",
    "PCAFunc(tpts_df2)\n",
    "summary(PCA_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "head(df_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not sure why it will not work for the lowest magnitude regions, I will need to ask my group for assistance with this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#It worked once for the bottom most regions but then it never worked again\n",
    "#importing the library for plotting PCA \n",
    "library(ggbiplot)\n",
    "\n",
    "#Created a list of PCAs for each bottom most region to iterate through\n",
    "pcalist = list(PCA_1,PCA_2,PCA_3,PCA_4,PCA_5,PCA_6,PCA_7,PCA_8,PCA_9,PCA_10)\n",
    "\n",
    "for (i in 1:length(pcalist)) {\n",
    "  print(ggbiplot(pcalist[[i]],alpha = 0.05))\n",
    "}\n",
    "\n",
    "#PC1 is Depth and PC2 is Mag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiple Linear Regression\n",
    "Now I will do multiple linear regression for the top magnitude and bottom magnitude regions. I will only perform the regression on regions that have most of their variables have a correlation rate over 10%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Elysa\n",
    "#Finds the unique values of the highest magnitude regions in the dataframe \n",
    "regions = unique(top_ten_mag_regions$newname)\n",
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I will make 10 dataframes with only numeric data for multiple linear regression for each highest magnitude region\n",
    "for (i in c(1,2,3,4,5,6,7,8,9,10)) {\n",
    "  assign(paste0(\"lindf_\", i), subset(bvalue_df, newname == regions[i],select=c(depth,mag,latitude,longitude,time_Year,time_Time,bvalue)) )\n",
    "}\n",
    "head(lindf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list to iterate through and do correlation for all variables\n",
    "linlist = list(lindf_1,lindf_2,lindf_3,lindf_4,lindf_5,lindf_6,lindf_7,lindf_8,lindf_9,lindf_10)\n",
    "\n",
    "for (i in 1:length(linlist)) {\n",
    "  print(\"Corr Matrix for Region:\",i)\n",
    "  print(cor(linlist[[i]]))\n",
    "}\n",
    "\n",
    "#After looking at correlation it appears that there is a lot of variation between regions on correlation \n",
    "#values for our predictor values magnitude. Correlation is strongest for corr matrix three, or Region 54"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(caret)\n",
    "#Multiple Linear Regression for Region 54\n",
    "#Based on the correlation matrix I will keep all variable in the analysis\n",
    "formula1 <- mag~.\n",
    "m1 <- train(formula1, lindf_3, method = \"lm\")\n",
    "summary(m1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to last time, even with the time and b-value added in the best I can do is an R^2 of 0.3272"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking to see what predicted values look like next to actual mag values \n",
    "coef.icept <- coef(m1$finalModel)[1] # define the intercept\n",
    "coef.slope <- coef(m1$finalModel)[2] # define the slope\n",
    "class_coefs <- coef(m1$finalModel)[2:8] # define adjusted slopes per class\n",
    "lindf_3$pred<-predict(m1, newdata=lindf_3) # find the predicted values of the dataset mag\n",
    "head(lindf_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to see if a logisitic regression would work better using region 54\n",
    "lindf_log = glm(mag ~ ., data=lindf_3, family=poisson)\n",
    "summary(lindf_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the accuracy of the model\n",
    "probs = predict(lindf_log, type = \"response\", newdata=lindf_3)\n",
    "preds <- ifelse(probs > 0.5,1,0)\n",
    "misClassificError <- mean (preds != probs)\n",
    "print(paste('Accuracy',1-misClassificError))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both linear and logistic regression do not seem to work well with the data and the b-values and time do not add to the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Findigng the bottom magnitude regions\n",
    "regions = unique(bottom_ten_mag_regions$newname)\n",
    "regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now I will make 10 dataframes with only numeric data for multiple linear regression for each bottom most region\n",
    "for (i in c(1,2,3,4,5,6,7,8,9,10)) {\n",
    "  assign(paste0(\"lindf_\", i), subset(bvalue_df, newname == regions[i],select=c(depth,mag,latitude,longitude,time_Year,time_Time,bvalue)) )\n",
    "}\n",
    "head(lindf_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a list to iterate through and do correlation for all variables\n",
    "linlist = list(lindf_1,lindf_2,lindf_3,lindf_4,lindf_5,lindf_6,lindf_7,lindf_8,lindf_9,lindf_10)\n",
    "\n",
    "for (i in 1:length(linlist)) {\n",
    "  print(\"Corr Matrix for Region:\",i)\n",
    "  print(cor(linlist[[i]]))\n",
    "}\n",
    "\n",
    "#After looking at correlation it appears that there is a lot of variation between regions on correlation \n",
    "#values for our predictor values magnitude. Correlation is strongest for corr matrix 10, or Region 387 and matrix3 or region 430"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple Linear Regression for Region 387\n",
    "#Based on the correlation matrix I will keep all variable in the analysis\n",
    "formula3 <- mag~.\n",
    "m3 <- train(formula3, lindf_3, method = \"lm\")\n",
    "summary(m3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to see if a logisitic regression would work better using region 387\n",
    "lindf_log = glm(mag ~ ., data=lindf_3, family=poisson)\n",
    "summary(lindf_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the accuracy of the model\n",
    "probs = predict(lindf_log, type = \"response\", newdata=lindf_3)\n",
    "preds <- ifelse(probs > 0.5,1,0)\n",
    "misClassificError <- mean (preds != probs)\n",
    "print(paste('Accuracy',1-misClassificError))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Multiple Linear Regression for Region 430\n",
    "#Based on the correlation matrix I will keep all variable in the analysis\n",
    "formula10 <- mag~.\n",
    "m10 <- train(formula3, lindf_10, method = \"lm\")\n",
    "summary(m10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trying to see if a logisitic regression would work better using region 430\n",
    "lindf_log = glm(mag ~ ., data=lindf_10, family=poisson)\n",
    "summary(lindf_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Checking the accuracy of the model\n",
    "probs = predict(lindf_log, type = \"response\", newdata=lindf_10)\n",
    "preds <- ifelse(probs > 0.5,1,0)\n",
    "misClassificError <- mean (preds != probs)\n",
    "print(paste('Accuracy',1-misClassificError))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think due to the lack of data for the bottom most regions makes it hard to use these regions for PCA. Both regions that showed high correlation did not work for multipe linear regression or logistic regrssion. Using the bottom most regions may not work for this type of analysis."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
